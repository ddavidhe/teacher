{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bf9c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidhe/Documents/GitHub/teacher/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e8438de",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_length = 1050\n",
    "footer_length = 250\n",
    "\n",
    "# given a PyTorch documentation URL, extract the main text and code blocks\n",
    "def extract_pytorch_docs(url):\n",
    "    page_text = \"\"\n",
    "    code_text = \"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Extract visible text\n",
    "            for tag in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]):\n",
    "                tag.decompose()\n",
    "\n",
    "            for node in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\"]):\n",
    "                text = node.get_text(separator=\" \", strip=True)\n",
    "                if text:\n",
    "                    page_text += (text + \" \")\n",
    "            \n",
    "\n",
    "            for code in soup.find_all(\"pre\"):\n",
    "                code_block = code.get_text(separator=\" \", strip=True)\n",
    "                lines = [line.strip() for line in code_block.split(\">>>\") if line.strip()]\n",
    "                cleaned_code = \"\\n\".join(lines)\n",
    "                code_text += (cleaned_code)\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return page_text[header_length:(len(page_text)-footer_length)], code_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0f8787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Last Updated On: Feb 26, 2025 PyTorch allows a tensor to be a View of an existing tensor. View tensor shares the same underlying data\n",
      "with its base tensor. Supporting View avoids explicit data copy, thus allows us to do fast and memory efficient\n",
      "reshaping, slicing and element-wise operations. For example, to get a view of an existing tensor t , you can call t.view(...) . Since views share underlying data with its base tensor, if you edit the data\n",
      "in the view, it will be reflected in the base tensor as well. Typically a PyTorch op returns a new tensor as output, e.g. add() .\n",
      "But in case of view ops, outputs are views of input tensors to avoid unnecessary data copy.\n",
      "No data movement occurs when creating a view, view tensor just changes the way\n",
      "it interprets the same data. Taking a view of contiguous tensor could potentially produce a non-contiguous tensor.\n",
      "Users should pay additional attention as contiguity might have implicit performance impact. transpose() is a common example. For reference, here’s a full list of view ops in PyTorch: Basic slicing and indexing op, e.g. tensor[0, 2:, 1:7:2] returns a view of base tensor , see note below. Basic slicing and indexing op, e.g. tensor[0, 2:, 1:7:2] returns a view of base tensor , see note below. adjoint() adjoint() as_strided() as_strided() detach() detach() diagonal() diagonal() expand() expand() expand_as() expand_as() movedim() movedim() narrow() narrow() permute() permute() select() select() squeeze() squeeze() transpose() transpose() t() t() T T H H mT mT mH mH real real imag imag view_as_real() view_as_real() unflatten() unflatten() unfold() unfold() unsqueeze() unsqueeze() view() view() view_as() view_as() unbind() unbind() split() split() hsplit() hsplit() vsplit() vsplit() tensor_split() tensor_split() split_with_sizes() split_with_sizes() swapaxes() swapaxes() swapdims() swapdims() chunk() chunk() indices() (sparse tensor only) indices() (sparse tensor only) values() (sparse tensor only) values() (sparse tensor only) Note When accessing the contents of a tensor via indexing, PyTorch follows Numpy behaviors\n",
      "that basic indexing returns views, while advanced indexing returns a copy.\n",
      "Assignment via either basic or advanced indexing is in-place. See more examples in Numpy indexing documentation . It’s also worth mentioning a few ops with special behaviors: reshape() , reshape_as() and flatten() can return either a view or new tensor, user code shouldn’t rely on whether it’s view or not. reshape() , reshape_as() and flatten() can return either a view or new tensor, user code shouldn’t rely on whether it’s view or not. contiguous() returns itself if input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data. contiguous() returns itself if input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data. For a more detailed walk-through of PyTorch internal implementation,\n",
      "please refer to ezyang’s blogpost about PyTorch Internals . torchao torchrec torchft TorchCode\n",
      "\n",
      "t = torch . rand ( 4 , 4 )\n",
      "b = t . view ( 2 , 8 )\n",
      "t . storage () . data_ptr () == b . storage () . data_ptr () # `t` and `b` share the same underlying data. True # Modifying view tensor changes base tensor as well.\n",
      "b [ 0 ][ 0 ] = 3.14\n",
      "t [ 0 ][ 0 ] tensor(3.14)base = torch . tensor ([[ 0 , 1 ],[ 2 , 3 ]])\n",
      "base . is_contiguous () True\n",
      "t = base . transpose ( 0 , 1 ) # `t` is a view of `base`. No data movement happened here. # View tensors might be non-contiguous.\n",
      "t . is_contiguous () False # To get a contiguous tensor, call `.contiguous()` to enforce # copying data when `t` is not contiguous.\n",
      "c = t . contiguous ()\n"
     ]
    }
   ],
   "source": [
    "url = \"https://docs.pytorch.org/docs/stable/tensor_view.html\"\n",
    "\n",
    "page_text, code_text = extract_pytorch_docs(url)\n",
    "\n",
    "print(page_text)\n",
    "print()\n",
    "print(code_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5560f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Last Updated On: Jun 12, 2025 This package provides a Future type that encapsulates\n",
      "an asynchronous execution and a set of utility functions to simplify operations\n",
      "on Future objects. Currently, the Future type is primarily used by the Distributed RPC Framework . Wrapper around a torch._C.Future which encapsulates an asynchronous\n",
      "execution of a callable, e.g. rpc_async() . It\n",
      "also exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future , which will be run\n",
      "when the Future is completed.  Multiple callbacks can be added to\n",
      "the same Future , but the order in which they will be executed cannot\n",
      "be guaranteed. The callback must take one argument, which is the\n",
      "reference to this Future . The callback function can use the value() method to get the value. Note that if this Future is\n",
      "already completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\n",
      "to synchronize after your callback has completed. add_done_callback can be cheaper if your callback does not return anything. But both then() and add_done_callback use the same callback\n",
      "registration API under the hood. With respect to GPU tensors, this method behaves in the same way as then() . callback ( Future ) – a Callable that takes in one argument,\n",
      "which is the reference to this Future . Note Note that if the callback function throws, either\n",
      "through the original future being completed with an exception and\n",
      "calling fut.wait() , or through other code in the callback,\n",
      "error handling must be carefully taken care of. For example, if\n",
      "this callback later completes additional futures, those futures are\n",
      "not marked as completed with an error and the user is responsible\n",
      "for handling completion/waiting on those futures independently. Example: Return True if this Future is done. A Future is done if it\n",
      "has a result or an exception. If the value contains tensors that reside on GPUs, Future.done() will return True even if the asynchronous kernels that are\n",
      "populating those tensors haven’t yet completed running on the device,\n",
      "because at such stage the result is already usable, provided one\n",
      "performs the appropriate synchronizations (see wait() ). bool Set an exception for this Future , which will mark this Future as\n",
      "completed with an error and trigger all attached callbacks. Note that\n",
      "when calling wait()/value() on this Future , the exception set here\n",
      "will be raised inline. result ( BaseException ) – the exception for this Future . Example: Set the result for this Future , which will mark this Future as\n",
      "completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\n",
      "called even if the asynchronous kernels that are populating those\n",
      "tensors haven’t yet completed running on the device, provided that the\n",
      "streams on which those kernels were enqueued are set as the current ones\n",
      "when this method is called. Put simply, it’s safe to call this method\n",
      "immediately after launching those kernels, without any additional\n",
      "synchronization, as long as one doesn’t change streams in between. This\n",
      "method will record events on all the relevant current streams and will\n",
      "use them to ensure proper scheduling for all the consumers of this Future . result ( object ) – the result object of this Future . Example: Append the given callback function to this Future , which will be run\n",
      "when the Future is completed.  Multiple callbacks can be added to\n",
      "the same Future , but the order in which they will be executed cannot\n",
      "be guaranteed (to enforce a certain order consider chaining: fut.then(cb1).then(cb2) ). The callback must take one argument, which\n",
      "is the reference to this Future . The callback function can use the value() method to get the value. Note that if this Future is\n",
      "already completed, the given callback will be run immediately inline. If the Future ’s value contains tensors that reside on GPUs, the\n",
      "callback might be invoked while the async kernels that are populating\n",
      "those tensors haven’t yet finished executing on the device. However, the\n",
      "callback will be invoked with some dedicated streams set as current\n",
      "(fetched from a global pool) which will be synchronized with those\n",
      "kernels. Hence any operation performed by the callback on these tensors\n",
      "will be scheduled on the device after the kernels complete. In other\n",
      "words, as long as the callback doesn’t switch streams, it can safely\n",
      "manipulate the result without any additional synchronization. This is\n",
      "similar to the non-blocking behavior of wait() . Similarly, if the callback returns a value that contains tensors that\n",
      "reside on a GPU, it can do so even if the kernels that are producing\n",
      "these tensors are still running on the device, as long as the callback\n",
      "didn’t change streams during its execution. If one wants to change\n",
      "streams, one must be careful to re-synchronize them with the original\n",
      "streams, that is, those that were current when the callback was invoked. callback ( Callable ) – a Callable that takes this Future as\n",
      "the only argument. A new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes. Future [ S ] Note Note that if the callback function throws, either\n",
      "through the original future being completed with an exception and\n",
      "calling fut.wait() , or through other code in the callback, the\n",
      "future returned by then will be marked appropriately with the\n",
      "encountered error. However, if this callback later completes\n",
      "additional futures, those futures are not marked as completed with\n",
      "an error and the user is responsible for handling completion/waiting\n",
      "on those futures independently. Example: Obtain the value of an already-completed future. This method should only be called after a call to wait() has\n",
      "completed, or inside a callback function passed to then() . In\n",
      "other cases this Future may not yet hold a value and calling value() could fail. If the value contains tensors that reside on GPUs, then this method will not perform any additional synchronization. This should be done\n",
      "beforehand, separately, through a call to wait() (except within\n",
      "callbacks, for which it’s already being taken care of by then() ). The value held by this Future . If the function (callback or RPC)\n",
      "creating the value has thrown an error, this value() method will\n",
      "also throw an error. T Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\n",
      "synchronization is performed with the kernels (executing on the device)\n",
      "which may be asynchronously populating those tensors. Such sync is\n",
      "non-blocking, which means that wait() will insert the necessary\n",
      "instructions in the current streams to ensure that further operations\n",
      "enqueued on those streams will be properly scheduled after the async\n",
      "kernels but, once that is done, wait() will return, even if those\n",
      "kernels are still running. No further synchronization is required when\n",
      "accessing and using the values, as long as one doesn’t change streams. The value held by this Future . If the function (callback or RPC)\n",
      "creating the value has thrown an error, this wait method will\n",
      "also throw an error. T Collects the provided Future objects into a single\n",
      "combined Future that is completed when all of the\n",
      "sub-futures are completed. futures ( list ) – a list of Future objects. Returns a Future object to a list of the passed\n",
      "in Futures. Future [ list [ torch.jit.Future ]] Waits for all provided futures to be complete, and returns\n",
      "the list of completed values. If any of the futures encounters an error,\n",
      "the method will exit early and report the error not waiting for other\n",
      "futures to complete. futures ( list ) – a list of Future object. A list of the completed Future results. This\n",
      "method will throw an error if wait on any Future throws. list torchao torchrec torchft TorchCode\n",
      "\n",
      "def callback ( fut ): ... print ( \"This will run after the future has finished.\" ) ... print ( fut . wait ())\n",
      "fut = torch . futures . Future ()\n",
      "fut . add_done_callback ( callback )\n",
      "fut . set_result ( 5 ) This will run after the future has finished. 5fut = torch . futures . Future ()\n",
      "fut . set_exception ( ValueError ( \"foo\" ))\n",
      "fut . wait () Traceback (most recent call last): ... ValueError : fooimport threading\n",
      "import time\n",
      "def slow_set_future ( fut , value ): ... time . sleep ( 0.5 ) ... fut . set_result ( value )\n",
      "fut = torch . futures . Future ()\n",
      "t = threading . Thread ( ... target = slow_set_future , ... args = ( fut , torch . ones ( 2 ) * 3 ) ... )\n",
      "t . start ()\n",
      "print ( fut . wait ()) tensor([3., 3.])\n",
      "t . join ()def callback ( fut ): ... print ( f \"RPC return value is { fut . wait () } .\" )\n",
      "fut = torch . futures . Future ()\n",
      "# The inserted callback will print the return value when\n",
      "# receiving the response from \"worker1\"\n",
      "cb_fut = fut . then ( callback )\n",
      "chain_cb_fut = cb_fut . then ( ... lambda x : print ( f \"Chained cb done. { x . wait () } \" ) ... )\n",
      "fut . set_result ( 5 ) RPC return value is 5. Chained cb done. Nonefut0 = torch . futures . Future ()\n",
      "fut1 = torch . futures . Future ()\n",
      "fut = torch . futures . collect_all ([ fut0 , fut1 ])\n",
      "fut0 . set_result ( 0 )\n",
      "fut1 . set_result ( 1 )\n",
      "fut_list = fut . wait ()\n",
      "print ( f \"fut0 result = { fut_list [ 0 ] . wait () } \" ) fut0 result = 0\n",
      "print ( f \"fut1 result = { fut_list [ 1 ] . wait () } \" ) fut1 result = 1\n"
     ]
    }
   ],
   "source": [
    "url = \"https://docs.pytorch.org/docs/stable/futures.html\"\n",
    "page_text, code_text = extract_pytorch_docs(url)\n",
    "\n",
    "print(page_text)\n",
    "print()\n",
    "print(code_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d69cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key = API_key)\n",
    "\n",
    "index_name = \"teacher\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index_for_model(\n",
    "        name=index_name,\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "        embed={\n",
    "            \"model\":\"llama-text-embed-v2\",\n",
    "            \"field_map\":{\"text\":\"chunk_text\"}\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceadc045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidhe/Documents/GitHub/teacher/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "HOST_NAME = os.getenv(\"HOST_NAME\")\n",
    "index = pc.Index(host=HOST_NAME)\n",
    "\n",
    "index.upsert_records(\n",
    "    \"__default__\",\n",
    "    [\n",
    "        {\n",
    "            \"_id\": \"futures_text\",\n",
    "            \"chunk_text\": page_text,\n",
    "        },\n",
    "        {\n",
    "            \"_id\": \"futures_code\",\n",
    "            \"chunk_text\": code_text,\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c27f49",
   "metadata": {},
   "source": [
    "# Next Steps.\n",
    "\n",
    "- Insert entire page of PyTorch into the db\n",
    "- Look into ways of scraping all PyTorch documentation better\n",
    "- Insert entire PyTorch into the db (lol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
